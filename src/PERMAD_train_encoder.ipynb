{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-12T07:56:09.059973300Z",
     "start_time": "2023-12-12T07:56:03.578070900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import locale\n",
    "import os\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from models.autoencoder import Autoencoder\n",
    "from data.PERMAD_data_reader import PermadData\n",
    "\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "# Use GPU: Send Tensors .to(device)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_subjects = PermadData().data_subjects"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T07:56:12.845735900Z",
     "start_time": "2023-12-12T07:56:12.783746700Z"
    }
   },
   "id": "1036612813834558"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :31\n",
      "Test  :10\n"
     ]
    }
   ],
   "source": [
    "idx_train, idx_test = train_test_split(range(0,len(data_subjects)), test_size=10)\n",
    "print(\"Train :\" + str(len(idx_train)))\n",
    "print(\"Test  :\" + str(len(idx_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T07:56:14.427771300Z",
     "start_time": "2023-12-12T07:56:14.403901200Z"
    }
   },
   "id": "5f34a6d1ac2f772c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class TrainingDS(Dataset):\n",
    "  def __init__(self, data_subjects, idx_train):\n",
    "    data = [data_subjects[i] for i in idx_train]  \n",
    "    self.healthy = [subject[:,0] for subject in data]\n",
    "    self.sick = [subject[:,-1] for subject in data]\n",
    "      \n",
    "  def __len__(self):\n",
    "    return(len(self.healthy))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return [self.healthy[idx], self.sick[idx]]\n",
    "\n",
    "train_ds = TrainingDS(data_subjects, idx_train)\n",
    "valid_ds = TrainingDS(data_subjects, idx_test)\n",
    "train_iterator = DataLoader(dataset=train_ds, batch_size=1, shuffle=True)\n",
    "valid_iterator = DataLoader(dataset=valid_ds, batch_size=1, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T07:56:15.630711100Z",
     "start_time": "2023-12-12T07:56:15.615072200Z"
    }
   },
   "id": "bc41beea489e4098"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, device, type=\"h\"):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    #for h,s in tqdm(iterator, desc=\"Training\", leave=False):\n",
    "    for h,s in iterator:\n",
    "        \n",
    "        if type==\"h\":\n",
    "            x = h.to(device).to(torch.float)\n",
    "        else:\n",
    "            x = s.to(device).to(torch.float)\n",
    "        \n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T07:56:17.067589400Z",
     "start_time": "2023-12-12T07:56:17.051743600Z"
    }
   },
   "id": "7d15970e6ca2ac59"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device, type=\"h\"):\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        #for h,s in tqdm(iterator, desc=\"Evaluating\", leave=False):\n",
    "        for (i, (h,s)) in enumerate(iterator):\n",
    "            \n",
    "            if type==\"h\":\n",
    "                x = h.to(device).to(torch.float)\n",
    "            else:\n",
    "                x = s.to(device).to(torch.float)\n",
    "\n",
    "            pred = model(x)\n",
    "            #if i==5 or i==7:\n",
    "            #    print(\"Encoder: \", model.encoder(x))\n",
    "            loss = criterion(pred, x)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T08:49:14.715152Z",
     "start_time": "2023-12-12T08:49:14.711008700Z"
    }
   },
   "id": "9b2b737a38ffc3b"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train_loop(model, train_iterator, valid_iterator, EPOCHS=5, type=\"h\", silent=True, schedular=None, optimizer=None):\n",
    "  best_valid_loss = float('inf')\n",
    "  best_train_loss = float('inf')\n",
    "\n",
    "  for epoch in trange(EPOCHS):\n",
    "      start_time = time.monotonic()\n",
    "\n",
    "      train_loss = train(model, train_iterator, optimizer, criterion, device, type=type)\n",
    "      schedular.step()\n",
    "      valid_loss = evaluate(model, valid_iterator, criterion, device, type=type)\n",
    "      \n",
    "      if train_loss < best_train_loss:\n",
    "          best_train_loss = train_loss\n",
    "\n",
    "      if valid_loss < best_valid_loss:\n",
    "          best_valid_loss = valid_loss\n",
    "          if type==\"h\":\n",
    "            torch.save(model.state_dict(), 'models/meta_parameters/autoencoder_param_healthy.pt')\n",
    "          else:\n",
    "            torch.save(model.state_dict(), 'models/meta_parameters/autoencoder_param_sick.pt')\n",
    "            \n",
    "      end_time = time.monotonic()\n",
    "\n",
    "      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "      if not silent:\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.6f} | Val. Loss: {valid_loss:.6f}')\n",
    "  return best_train_loss, best_valid_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T07:56:18.887872500Z",
     "start_time": "2023-12-12T07:56:18.878639400Z"
    }
   },
   "id": "b09b96b5620d8f96"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/200 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3178a2cdc99d42128f13628a29c53f06"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.996510 | Val. Loss: 0.984339\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.946828 | Val. Loss: 0.816622\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.479641 | Val. Loss: 0.213997\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.220058 | Val. Loss: 0.138704\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.148757 | Val. Loss: 0.095549\n",
      "Epoch: 06 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.116163 | Val. Loss: 0.070758\n",
      "Epoch: 07 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.100950 | Val. Loss: 0.061761\n",
      "Epoch: 08 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.096668 | Val. Loss: 0.060864\n",
      "Epoch: 09 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.093053 | Val. Loss: 0.062897\n",
      "Epoch: 10 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.090365 | Val. Loss: 0.058832\n",
      "Epoch: 11 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.089493 | Val. Loss: 0.058140\n",
      "Epoch: 12 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.088990 | Val. Loss: 0.060085\n",
      "Epoch: 13 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.088595 | Val. Loss: 0.059297\n",
      "Epoch: 14 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.087624 | Val. Loss: 0.059636\n",
      "Epoch: 15 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.087556 | Val. Loss: 0.059398\n",
      "Epoch: 16 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.087865 | Val. Loss: 0.061588\n",
      "Epoch: 17 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.087471 | Val. Loss: 0.060016\n",
      "Epoch: 18 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.088324 | Val. Loss: 0.056925\n",
      "Epoch: 19 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.087492 | Val. Loss: 0.059990\n",
      "Epoch: 20 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086711 | Val. Loss: 0.057429\n",
      "Epoch: 21 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086641 | Val. Loss: 0.059752\n",
      "Epoch: 22 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086995 | Val. Loss: 0.058899\n",
      "Epoch: 23 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086708 | Val. Loss: 0.060146\n",
      "Epoch: 24 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.087131 | Val. Loss: 0.059086\n",
      "Epoch: 25 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086585 | Val. Loss: 0.059755\n",
      "Epoch: 26 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086069 | Val. Loss: 0.056704\n",
      "Epoch: 27 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086404 | Val. Loss: 0.058737\n",
      "Epoch: 28 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086097 | Val. Loss: 0.057590\n",
      "Epoch: 29 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086359 | Val. Loss: 0.058071\n",
      "Epoch: 30 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.087148 | Val. Loss: 0.056116\n",
      "Epoch: 31 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.087300 | Val. Loss: 0.058335\n",
      "Epoch: 32 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085659 | Val. Loss: 0.056922\n",
      "Epoch: 33 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085728 | Val. Loss: 0.057295\n",
      "Epoch: 34 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086403 | Val. Loss: 0.055909\n",
      "Epoch: 35 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086312 | Val. Loss: 0.063171\n",
      "Epoch: 36 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.087314 | Val. Loss: 0.058541\n",
      "Epoch: 37 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085646 | Val. Loss: 0.057288\n",
      "Epoch: 38 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085103 | Val. Loss: 0.056865\n",
      "Epoch: 39 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085600 | Val. Loss: 0.060891\n",
      "Epoch: 40 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085299 | Val. Loss: 0.057881\n",
      "Epoch: 41 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.087589 | Val. Loss: 0.060158\n",
      "Epoch: 42 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.087788 | Val. Loss: 0.061776\n",
      "Epoch: 43 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085237 | Val. Loss: 0.057679\n",
      "Epoch: 44 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085353 | Val. Loss: 0.061958\n",
      "Epoch: 45 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086162 | Val. Loss: 0.056279\n",
      "Epoch: 46 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086398 | Val. Loss: 0.062327\n",
      "Epoch: 47 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085801 | Val. Loss: 0.058266\n",
      "Epoch: 48 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084957 | Val. Loss: 0.053029\n",
      "Epoch: 49 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086782 | Val. Loss: 0.063260\n",
      "Epoch: 50 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086425 | Val. Loss: 0.057956\n",
      "Epoch: 51 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085462 | Val. Loss: 0.057045\n",
      "Epoch: 52 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086252 | Val. Loss: 0.061985\n",
      "Epoch: 53 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086421 | Val. Loss: 0.057606\n",
      "Epoch: 54 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085969 | Val. Loss: 0.058287\n",
      "Epoch: 55 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085490 | Val. Loss: 0.056913\n",
      "Epoch: 56 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084566 | Val. Loss: 0.059921\n",
      "Epoch: 57 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083969 | Val. Loss: 0.055765\n",
      "Epoch: 58 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084520 | Val. Loss: 0.054731\n",
      "Epoch: 59 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085290 | Val. Loss: 0.054355\n",
      "Epoch: 60 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084618 | Val. Loss: 0.054458\n",
      "Epoch: 61 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085452 | Val. Loss: 0.056903\n",
      "Epoch: 62 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086080 | Val. Loss: 0.055698\n",
      "Epoch: 63 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085836 | Val. Loss: 0.057113\n",
      "Epoch: 64 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086555 | Val. Loss: 0.061450\n",
      "Epoch: 65 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085267 | Val. Loss: 0.061817\n",
      "Epoch: 66 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084425 | Val. Loss: 0.058284\n",
      "Epoch: 67 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083992 | Val. Loss: 0.056155\n",
      "Epoch: 68 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083326 | Val. Loss: 0.066856\n",
      "Epoch: 69 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086330 | Val. Loss: 0.061587\n",
      "Epoch: 70 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084495 | Val. Loss: 0.060356\n",
      "Epoch: 71 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083940 | Val. Loss: 0.053639\n",
      "Epoch: 72 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084337 | Val. Loss: 0.061388\n",
      "Epoch: 73 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084843 | Val. Loss: 0.062327\n",
      "Epoch: 74 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085756 | Val. Loss: 0.066325\n",
      "Epoch: 75 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085538 | Val. Loss: 0.060559\n",
      "Epoch: 76 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084906 | Val. Loss: 0.057892\n",
      "Epoch: 77 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083563 | Val. Loss: 0.056134\n",
      "Epoch: 78 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083334 | Val. Loss: 0.062082\n",
      "Epoch: 79 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083985 | Val. Loss: 0.054904\n",
      "Epoch: 80 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083941 | Val. Loss: 0.060061\n",
      "Epoch: 81 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083175 | Val. Loss: 0.060153\n",
      "Epoch: 82 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083752 | Val. Loss: 0.061511\n",
      "Epoch: 83 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083367 | Val. Loss: 0.058746\n",
      "Epoch: 84 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082778 | Val. Loss: 0.057621\n",
      "Epoch: 85 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082953 | Val. Loss: 0.059048\n",
      "Epoch: 86 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083104 | Val. Loss: 0.064996\n",
      "Epoch: 87 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083871 | Val. Loss: 0.063566\n",
      "Epoch: 88 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082909 | Val. Loss: 0.058787\n",
      "Epoch: 89 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083096 | Val. Loss: 0.059194\n",
      "Epoch: 90 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083255 | Val. Loss: 0.062517\n",
      "Epoch: 91 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082992 | Val. Loss: 0.067471\n",
      "Epoch: 92 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083701 | Val. Loss: 0.063836\n",
      "Epoch: 93 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084374 | Val. Loss: 0.061388\n",
      "Epoch: 94 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083143 | Val. Loss: 0.064071\n",
      "Epoch: 95 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083950 | Val. Loss: 0.064325\n",
      "Epoch: 96 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084188 | Val. Loss: 0.063383\n",
      "Epoch: 97 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084849 | Val. Loss: 0.064442\n",
      "Epoch: 98 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083960 | Val. Loss: 0.057298\n",
      "Epoch: 99 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084577 | Val. Loss: 0.058894\n",
      "Epoch: 100 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082621 | Val. Loss: 0.060116\n",
      "Epoch: 101 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082988 | Val. Loss: 0.062048\n",
      "Epoch: 102 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083150 | Val. Loss: 0.065107\n",
      "Epoch: 103 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083204 | Val. Loss: 0.059120\n",
      "Epoch: 104 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083447 | Val. Loss: 0.061750\n",
      "Epoch: 105 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083024 | Val. Loss: 0.059228\n",
      "Epoch: 106 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082987 | Val. Loss: 0.063850\n",
      "Epoch: 107 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083495 | Val. Loss: 0.066446\n",
      "Epoch: 108 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083081 | Val. Loss: 0.068406\n",
      "Epoch: 109 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086076 | Val. Loss: 0.067267\n",
      "Epoch: 110 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.087148 | Val. Loss: 0.059803\n",
      "Epoch: 111 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.089585 | Val. Loss: 0.065030\n",
      "Epoch: 112 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086150 | Val. Loss: 0.074191\n",
      "Epoch: 113 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.089645 | Val. Loss: 0.075004\n",
      "Epoch: 114 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086352 | Val. Loss: 0.057281\n",
      "Epoch: 115 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085823 | Val. Loss: 0.066585\n",
      "Epoch: 116 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084155 | Val. Loss: 0.065376\n",
      "Epoch: 117 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083642 | Val. Loss: 0.066698\n",
      "Epoch: 118 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085463 | Val. Loss: 0.066638\n",
      "Epoch: 119 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084332 | Val. Loss: 0.055698\n",
      "Epoch: 120 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082840 | Val. Loss: 0.062808\n",
      "Epoch: 121 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083658 | Val. Loss: 0.061873\n",
      "Epoch: 122 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082917 | Val. Loss: 0.063664\n",
      "Epoch: 123 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082609 | Val. Loss: 0.062696\n",
      "Epoch: 124 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082728 | Val. Loss: 0.069360\n",
      "Epoch: 125 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083026 | Val. Loss: 0.060613\n",
      "Epoch: 126 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083149 | Val. Loss: 0.063913\n",
      "Epoch: 127 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082518 | Val. Loss: 0.063370\n",
      "Epoch: 128 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082781 | Val. Loss: 0.069494\n",
      "Epoch: 129 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082521 | Val. Loss: 0.062342\n",
      "Epoch: 130 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082507 | Val. Loss: 0.065129\n",
      "Epoch: 131 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082425 | Val. Loss: 0.065787\n",
      "Epoch: 132 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082505 | Val. Loss: 0.065010\n",
      "Epoch: 133 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082615 | Val. Loss: 0.060779\n",
      "Epoch: 134 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084246 | Val. Loss: 0.069798\n",
      "Epoch: 135 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082762 | Val. Loss: 0.065888\n",
      "Epoch: 136 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082370 | Val. Loss: 0.061428\n",
      "Epoch: 137 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082372 | Val. Loss: 0.064702\n",
      "Epoch: 138 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082730 | Val. Loss: 0.065888\n",
      "Epoch: 139 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083116 | Val. Loss: 0.065803\n",
      "Epoch: 140 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082227 | Val. Loss: 0.065984\n",
      "Epoch: 141 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082605 | Val. Loss: 0.070230\n",
      "Epoch: 142 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082948 | Val. Loss: 0.072284\n",
      "Epoch: 143 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.087337 | Val. Loss: 0.076431\n",
      "Epoch: 144 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085981 | Val. Loss: 0.068861\n",
      "Epoch: 145 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084172 | Val. Loss: 0.069384\n",
      "Epoch: 146 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083139 | Val. Loss: 0.070122\n",
      "Epoch: 147 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083487 | Val. Loss: 0.070024\n",
      "Epoch: 148 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082235 | Val. Loss: 0.067112\n",
      "Epoch: 149 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082042 | Val. Loss: 0.065149\n",
      "Epoch: 150 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082715 | Val. Loss: 0.072956\n",
      "Epoch: 151 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083795 | Val. Loss: 0.064548\n",
      "Epoch: 152 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083132 | Val. Loss: 0.070955\n",
      "Epoch: 153 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084874 | Val. Loss: 0.067572\n",
      "Epoch: 154 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083591 | Val. Loss: 0.063343\n",
      "Epoch: 155 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082798 | Val. Loss: 0.070932\n",
      "Epoch: 156 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083349 | Val. Loss: 0.066985\n",
      "Epoch: 157 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083023 | Val. Loss: 0.065397\n",
      "Epoch: 158 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083133 | Val. Loss: 0.072846\n",
      "Epoch: 159 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082998 | Val. Loss: 0.067469\n",
      "Epoch: 160 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082072 | Val. Loss: 0.067186\n",
      "Epoch: 161 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082129 | Val. Loss: 0.065297\n",
      "Epoch: 162 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082340 | Val. Loss: 0.069248\n",
      "Epoch: 163 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082282 | Val. Loss: 0.067432\n",
      "Epoch: 164 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083077 | Val. Loss: 0.066933\n",
      "Epoch: 165 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081633 | Val. Loss: 0.068420\n",
      "Epoch: 166 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081552 | Val. Loss: 0.069635\n",
      "Epoch: 167 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081594 | Val. Loss: 0.067136\n",
      "Epoch: 168 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082023 | Val. Loss: 0.068949\n",
      "Epoch: 169 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081526 | Val. Loss: 0.072385\n",
      "Epoch: 170 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081866 | Val. Loss: 0.067005\n",
      "Epoch: 171 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081178 | Val. Loss: 0.064812\n",
      "Epoch: 172 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082576 | Val. Loss: 0.067319\n",
      "Epoch: 173 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.088715 | Val. Loss: 0.069054\n",
      "Epoch: 174 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083993 | Val. Loss: 0.073602\n",
      "Epoch: 175 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084031 | Val. Loss: 0.060082\n",
      "Epoch: 176 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082548 | Val. Loss: 0.067908\n",
      "Epoch: 177 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082017 | Val. Loss: 0.066657\n",
      "Epoch: 178 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082318 | Val. Loss: 0.068504\n",
      "Epoch: 179 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082783 | Val. Loss: 0.071482\n",
      "Epoch: 180 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081774 | Val. Loss: 0.068469\n",
      "Epoch: 181 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081456 | Val. Loss: 0.065906\n",
      "Epoch: 182 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081522 | Val. Loss: 0.068701\n",
      "Epoch: 183 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082756 | Val. Loss: 0.069152\n",
      "Epoch: 184 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082048 | Val. Loss: 0.069327\n",
      "Epoch: 185 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081716 | Val. Loss: 0.068269\n",
      "Epoch: 186 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081632 | Val. Loss: 0.071887\n",
      "Epoch: 187 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081281 | Val. Loss: 0.069795\n",
      "Epoch: 188 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081426 | Val. Loss: 0.071289\n",
      "Epoch: 189 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081793 | Val. Loss: 0.071693\n",
      "Epoch: 190 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083100 | Val. Loss: 0.077366\n",
      "Epoch: 191 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084314 | Val. Loss: 0.077215\n",
      "Epoch: 192 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084145 | Val. Loss: 0.076674\n",
      "Epoch: 193 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082660 | Val. Loss: 0.065430\n",
      "Epoch: 194 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081456 | Val. Loss: 0.067473\n",
      "Epoch: 195 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082876 | Val. Loss: 0.075969\n",
      "Epoch: 196 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085251 | Val. Loss: 0.063526\n",
      "Epoch: 197 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083013 | Val. Loss: 0.077800\n",
      "Epoch: 198 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084522 | Val. Loss: 0.065513\n",
      "Epoch: 199 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083275 | Val. Loss: 0.067502\n",
      "Epoch: 200 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084618 | Val. Loss: 0.079836\n",
      "Model h: train 0.08118 | valid 0.05303\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/200 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11b5a9c8b3234839be8b1413270f683d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 1.015127 | Val. Loss: 1.007268\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.992796 | Val. Loss: 0.987289\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.973052 | Val. Loss: 0.967052\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.950036 | Val. Loss: 0.938775\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.907083 | Val. Loss: 0.869636\n",
      "Epoch: 06 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.826814 | Val. Loss: 0.756329\n",
      "Epoch: 07 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.699414 | Val. Loss: 0.607526\n",
      "Epoch: 08 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.557451 | Val. Loss: 0.456468\n",
      "Epoch: 09 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.448180 | Val. Loss: 0.351551\n",
      "Epoch: 10 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.394237 | Val. Loss: 0.299780\n",
      "Epoch: 11 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.374896 | Val. Loss: 0.269354\n",
      "Epoch: 12 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.361366 | Val. Loss: 0.253028\n",
      "Epoch: 13 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.348524 | Val. Loss: 0.244921\n",
      "Epoch: 14 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.339787 | Val. Loss: 0.232544\n",
      "Epoch: 15 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.330197 | Val. Loss: 0.219346\n",
      "Epoch: 16 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.321992 | Val. Loss: 0.208477\n",
      "Epoch: 17 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.313527 | Val. Loss: 0.198096\n",
      "Epoch: 18 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.303539 | Val. Loss: 0.189594\n",
      "Epoch: 19 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.292697 | Val. Loss: 0.181986\n",
      "Epoch: 20 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.279630 | Val. Loss: 0.169791\n",
      "Epoch: 21 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.265341 | Val. Loss: 0.158490\n",
      "Epoch: 22 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.250268 | Val. Loss: 0.147066\n",
      "Epoch: 23 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.234734 | Val. Loss: 0.135528\n",
      "Epoch: 24 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.219412 | Val. Loss: 0.125320\n",
      "Epoch: 25 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.203429 | Val. Loss: 0.115406\n",
      "Epoch: 26 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.187386 | Val. Loss: 0.101795\n",
      "Epoch: 27 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.171169 | Val. Loss: 0.093981\n",
      "Epoch: 28 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.155859 | Val. Loss: 0.084739\n",
      "Epoch: 29 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.142910 | Val. Loss: 0.075871\n",
      "Epoch: 30 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.131463 | Val. Loss: 0.068903\n",
      "Epoch: 31 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.121267 | Val. Loss: 0.062740\n",
      "Epoch: 32 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.112799 | Val. Loss: 0.058251\n",
      "Epoch: 33 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.106469 | Val. Loss: 0.054763\n",
      "Epoch: 34 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.101451 | Val. Loss: 0.052397\n",
      "Epoch: 35 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.097752 | Val. Loss: 0.050281\n",
      "Epoch: 36 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.094565 | Val. Loss: 0.049378\n",
      "Epoch: 37 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.092838 | Val. Loss: 0.048558\n",
      "Epoch: 38 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.091223 | Val. Loss: 0.047896\n",
      "Epoch: 39 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.090134 | Val. Loss: 0.047196\n",
      "Epoch: 40 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.089334 | Val. Loss: 0.046958\n",
      "Epoch: 41 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.088857 | Val. Loss: 0.046932\n",
      "Epoch: 42 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.088021 | Val. Loss: 0.046409\n",
      "Epoch: 43 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.087833 | Val. Loss: 0.046181\n",
      "Epoch: 44 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.087636 | Val. Loss: 0.046085\n",
      "Epoch: 45 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086896 | Val. Loss: 0.045953\n",
      "Epoch: 46 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086683 | Val. Loss: 0.046162\n",
      "Epoch: 47 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086312 | Val. Loss: 0.045624\n",
      "Epoch: 48 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.086012 | Val. Loss: 0.045672\n",
      "Epoch: 49 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085776 | Val. Loss: 0.045586\n",
      "Epoch: 50 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085440 | Val. Loss: 0.045491\n",
      "Epoch: 51 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085341 | Val. Loss: 0.045363\n",
      "Epoch: 52 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.085154 | Val. Loss: 0.045340\n",
      "Epoch: 53 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084971 | Val. Loss: 0.045460\n",
      "Epoch: 54 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084940 | Val. Loss: 0.044720\n",
      "Epoch: 55 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084385 | Val. Loss: 0.044900\n",
      "Epoch: 56 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084459 | Val. Loss: 0.045099\n",
      "Epoch: 57 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084137 | Val. Loss: 0.044748\n",
      "Epoch: 58 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.084105 | Val. Loss: 0.044299\n",
      "Epoch: 59 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083979 | Val. Loss: 0.044972\n",
      "Epoch: 60 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083420 | Val. Loss: 0.044574\n",
      "Epoch: 61 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083172 | Val. Loss: 0.044672\n",
      "Epoch: 62 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083680 | Val. Loss: 0.044949\n",
      "Epoch: 63 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.083328 | Val. Loss: 0.044720\n",
      "Epoch: 64 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082878 | Val. Loss: 0.044681\n",
      "Epoch: 65 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082938 | Val. Loss: 0.044835\n",
      "Epoch: 66 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082902 | Val. Loss: 0.044462\n",
      "Epoch: 67 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082724 | Val. Loss: 0.044546\n",
      "Epoch: 68 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082336 | Val. Loss: 0.044672\n",
      "Epoch: 69 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082286 | Val. Loss: 0.045000\n",
      "Epoch: 70 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082112 | Val. Loss: 0.044767\n",
      "Epoch: 71 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082078 | Val. Loss: 0.044639\n",
      "Epoch: 72 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.082008 | Val. Loss: 0.044607\n",
      "Epoch: 73 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081938 | Val. Loss: 0.044805\n",
      "Epoch: 74 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081737 | Val. Loss: 0.044498\n",
      "Epoch: 75 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081694 | Val. Loss: 0.044567\n",
      "Epoch: 76 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081606 | Val. Loss: 0.044658\n",
      "Epoch: 77 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081490 | Val. Loss: 0.044848\n",
      "Epoch: 78 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081680 | Val. Loss: 0.044874\n",
      "Epoch: 79 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081406 | Val. Loss: 0.044745\n",
      "Epoch: 80 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081049 | Val. Loss: 0.044972\n",
      "Epoch: 81 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081006 | Val. Loss: 0.044540\n",
      "Epoch: 82 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.080846 | Val. Loss: 0.044800\n",
      "Epoch: 83 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.080871 | Val. Loss: 0.045021\n",
      "Epoch: 84 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081217 | Val. Loss: 0.044524\n",
      "Epoch: 85 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.081005 | Val. Loss: 0.044919\n",
      "Epoch: 86 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.080573 | Val. Loss: 0.044640\n",
      "Epoch: 87 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.080407 | Val. Loss: 0.044701\n",
      "Epoch: 88 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.080482 | Val. Loss: 0.044830\n",
      "Epoch: 89 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.080022 | Val. Loss: 0.044784\n",
      "Epoch: 90 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.080508 | Val. Loss: 0.045094\n",
      "Epoch: 91 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.080028 | Val. Loss: 0.044754\n",
      "Epoch: 92 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.079990 | Val. Loss: 0.044827\n",
      "Epoch: 93 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.080106 | Val. Loss: 0.044602\n",
      "Epoch: 94 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.080000 | Val. Loss: 0.045151\n",
      "Epoch: 95 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.079389 | Val. Loss: 0.044830\n",
      "Epoch: 96 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.079699 | Val. Loss: 0.044581\n",
      "Epoch: 97 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.079683 | Val. Loss: 0.045095\n",
      "Epoch: 98 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.079161 | Val. Loss: 0.044746\n",
      "Epoch: 99 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.078986 | Val. Loss: 0.044552\n",
      "Epoch: 100 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.079271 | Val. Loss: 0.044596\n",
      "Epoch: 101 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.079194 | Val. Loss: 0.044632\n",
      "Epoch: 102 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.078700 | Val. Loss: 0.044854\n",
      "Epoch: 103 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.078726 | Val. Loss: 0.045113\n",
      "Epoch: 104 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.078467 | Val. Loss: 0.045000\n",
      "Epoch: 105 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.078561 | Val. Loss: 0.044954\n",
      "Epoch: 106 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.078132 | Val. Loss: 0.044542\n",
      "Epoch: 107 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.077990 | Val. Loss: 0.044679\n",
      "Epoch: 108 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.077766 | Val. Loss: 0.045009\n",
      "Epoch: 109 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.077732 | Val. Loss: 0.044588\n",
      "Epoch: 110 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.077410 | Val. Loss: 0.044717\n",
      "Epoch: 111 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.077500 | Val. Loss: 0.044583\n",
      "Epoch: 112 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.077351 | Val. Loss: 0.044733\n",
      "Epoch: 113 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.077170 | Val. Loss: 0.044765\n",
      "Epoch: 114 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.077083 | Val. Loss: 0.045023\n",
      "Epoch: 115 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.076902 | Val. Loss: 0.044900\n",
      "Epoch: 116 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.076765 | Val. Loss: 0.044628\n",
      "Epoch: 117 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.076769 | Val. Loss: 0.044884\n",
      "Epoch: 118 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.076389 | Val. Loss: 0.044900\n",
      "Epoch: 119 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.076554 | Val. Loss: 0.044697\n",
      "Epoch: 120 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.076302 | Val. Loss: 0.044474\n",
      "Epoch: 121 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.076015 | Val. Loss: 0.044590\n",
      "Epoch: 122 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.075868 | Val. Loss: 0.045117\n",
      "Epoch: 123 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.075680 | Val. Loss: 0.044906\n",
      "Epoch: 124 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.075542 | Val. Loss: 0.044939\n",
      "Epoch: 125 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.075618 | Val. Loss: 0.044851\n",
      "Epoch: 126 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.074956 | Val. Loss: 0.044829\n",
      "Epoch: 127 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.075036 | Val. Loss: 0.044746\n",
      "Epoch: 128 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.074688 | Val. Loss: 0.044587\n",
      "Epoch: 129 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.074685 | Val. Loss: 0.044472\n",
      "Epoch: 130 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.074618 | Val. Loss: 0.044707\n",
      "Epoch: 131 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.074140 | Val. Loss: 0.044760\n",
      "Epoch: 132 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.073975 | Val. Loss: 0.044467\n",
      "Epoch: 133 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.073889 | Val. Loss: 0.044429\n",
      "Epoch: 134 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.073653 | Val. Loss: 0.044554\n",
      "Epoch: 135 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.073892 | Val. Loss: 0.044774\n",
      "Epoch: 136 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.073219 | Val. Loss: 0.044524\n",
      "Epoch: 137 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.073164 | Val. Loss: 0.044687\n",
      "Epoch: 138 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.073037 | Val. Loss: 0.044756\n",
      "Epoch: 139 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.072787 | Val. Loss: 0.044643\n",
      "Epoch: 140 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.072573 | Val. Loss: 0.044625\n",
      "Epoch: 141 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.072252 | Val. Loss: 0.044618\n",
      "Epoch: 142 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.072129 | Val. Loss: 0.044357\n",
      "Epoch: 143 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.071960 | Val. Loss: 0.044473\n",
      "Epoch: 144 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.071853 | Val. Loss: 0.044366\n",
      "Epoch: 145 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.071731 | Val. Loss: 0.044232\n",
      "Epoch: 146 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.071667 | Val. Loss: 0.043985\n",
      "Epoch: 147 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.071302 | Val. Loss: 0.044625\n",
      "Epoch: 148 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.071135 | Val. Loss: 0.044601\n",
      "Epoch: 149 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.070965 | Val. Loss: 0.044435\n",
      "Epoch: 150 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.070805 | Val. Loss: 0.044280\n",
      "Epoch: 151 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.070635 | Val. Loss: 0.044372\n",
      "Epoch: 152 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.070540 | Val. Loss: 0.044600\n",
      "Epoch: 153 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.070424 | Val. Loss: 0.044102\n",
      "Epoch: 154 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.070223 | Val. Loss: 0.044434\n",
      "Epoch: 155 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.070178 | Val. Loss: 0.044328\n",
      "Epoch: 156 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.070156 | Val. Loss: 0.044202\n",
      "Epoch: 157 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.070001 | Val. Loss: 0.045083\n",
      "Epoch: 158 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.069710 | Val. Loss: 0.044222\n",
      "Epoch: 159 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.069419 | Val. Loss: 0.044105\n",
      "Epoch: 160 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.069372 | Val. Loss: 0.044035\n",
      "Epoch: 161 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.069387 | Val. Loss: 0.044312\n",
      "Epoch: 162 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.069098 | Val. Loss: 0.044261\n",
      "Epoch: 163 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.069076 | Val. Loss: 0.044580\n",
      "Epoch: 164 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.069164 | Val. Loss: 0.043703\n",
      "Epoch: 165 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.068773 | Val. Loss: 0.043959\n",
      "Epoch: 166 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.068588 | Val. Loss: 0.044430\n",
      "Epoch: 167 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.068580 | Val. Loss: 0.044401\n",
      "Epoch: 168 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.068441 | Val. Loss: 0.044545\n",
      "Epoch: 169 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.068235 | Val. Loss: 0.043997\n",
      "Epoch: 170 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.068193 | Val. Loss: 0.044408\n",
      "Epoch: 171 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.067958 | Val. Loss: 0.044116\n",
      "Epoch: 172 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.068010 | Val. Loss: 0.043982\n",
      "Epoch: 173 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.067684 | Val. Loss: 0.043742\n",
      "Epoch: 174 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.067727 | Val. Loss: 0.043646\n",
      "Epoch: 175 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.067583 | Val. Loss: 0.043898\n",
      "Epoch: 176 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.067738 | Val. Loss: 0.044285\n",
      "Epoch: 177 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.067305 | Val. Loss: 0.043821\n",
      "Epoch: 178 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.067293 | Val. Loss: 0.044235\n",
      "Epoch: 179 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.067012 | Val. Loss: 0.043909\n",
      "Epoch: 180 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.066890 | Val. Loss: 0.044271\n",
      "Epoch: 181 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.066958 | Val. Loss: 0.044213\n",
      "Epoch: 182 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.067049 | Val. Loss: 0.043608\n",
      "Epoch: 183 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.066845 | Val. Loss: 0.043978\n",
      "Epoch: 184 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.066936 | Val. Loss: 0.044100\n",
      "Epoch: 185 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.066977 | Val. Loss: 0.044220\n",
      "Epoch: 186 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.066528 | Val. Loss: 0.043805\n",
      "Epoch: 187 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.066464 | Val. Loss: 0.043918\n",
      "Epoch: 188 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.066258 | Val. Loss: 0.043887\n",
      "Epoch: 189 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.066388 | Val. Loss: 0.043833\n",
      "Epoch: 190 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.066318 | Val. Loss: 0.043822\n",
      "Epoch: 191 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.066315 | Val. Loss: 0.044227\n",
      "Epoch: 192 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.066101 | Val. Loss: 0.043476\n",
      "Epoch: 193 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.065937 | Val. Loss: 0.043999\n",
      "Epoch: 194 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.066000 | Val. Loss: 0.043955\n",
      "Epoch: 195 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.065940 | Val. Loss: 0.044029\n",
      "Epoch: 196 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.065758 | Val. Loss: 0.043628\n",
      "Epoch: 197 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.065679 | Val. Loss: 0.043887\n",
      "Epoch: 198 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.065572 | Val. Loss: 0.043654\n",
      "Epoch: 199 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.065407 | Val. Loss: 0.043961\n",
      "Epoch: 200 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.065584 | Val. Loss: 0.043941\n",
      "Model s: train 0.06541 | valid 0.04348\n"
     ]
    }
   ],
   "source": [
    "model_h = Autoencoder(features=98, layers=[16,4], dropout=0.0).to(device)\n",
    "model_s = Autoencoder(features=98, layers=[16,4], dropout=0.0).to(device)\n",
    "epochs = 100\n",
    "\n",
    "optimizer_h = torch.optim.Adam(model_h.parameters(), lr=0.003)\n",
    "schedular_h = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_h, T_max = epochs*len(train_ds))\n",
    "optimizer_s = torch.optim.Adam(model_s.parameters(), lr=0.0003)\n",
    "schedular_s = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_s, T_max = epochs*len(train_ds))\n",
    "#criterion = nn.MSELoss(reduction=\"sum\")\n",
    "criterion = nn.MSELoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "best_train_loss, best_valid_loss = train_loop(model_h, train_iterator, valid_iterator, EPOCHS=200, type=\"h\", silent=False, schedular=schedular_h, optimizer=optimizer_h)\n",
    "print(f'Model h: train {best_train_loss:.5f} | valid {best_valid_loss:.5f}')\n",
    "\n",
    "best_train_loss, best_valid_loss = train_loop(model_s, train_iterator, valid_iterator, EPOCHS=200, type=\"s\", silent=False, schedular=schedular_s, optimizer=optimizer_s)\n",
    "print(f'Model s: train {best_train_loss:.5f} | valid {best_valid_loss:.5f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T08:49:37.620693Z",
     "start_time": "2023-12-12T08:49:27.731153400Z"
    }
   },
   "id": "e201806fc7695b62"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(5.1171, grad_fn=<AddBackward0>)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h,s = valid_ds[0]\n",
    "x = torch.tensor(s).to(device).to(torch.float)\n",
    "x_pred = model_h(x)\n",
    "sum(x_pred - x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T12:48:11.211198100Z",
     "start_time": "2023-12-11T12:48:11.194842900Z"
    }
   },
   "id": "4e01087222c93cfd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d7f5545827efc22b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
